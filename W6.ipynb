{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP 4437 Week 6\n",
    "### Scroll down for the questions. ðŸ ‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io.arff import loadarff\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load data\n",
    "raw_data_train = loadarff('emotions-train.arff')\n",
    "raw_data_test = loadarff('emotions-test.arff')\n",
    "\n",
    "\n",
    "df_train = pd.DataFrame(raw_data_train[0]).astype(float)\n",
    "df_test = pd.DataFrame(raw_data[0]).astype(float)\n",
    "\n",
    "\n",
    "labels = ['amazed-suprised', 'happy-pleased', 'relaxing-calm', 'quiet-still', 'sad-lonely', 'angry-aggresive']\n",
    "\n",
    "X_train = df_train.drop(labels, axis=1)\n",
    "Y_train = df_train[labels]\n",
    "\n",
    "X_train_np = X_train.to_numpy()\n",
    "Y_train_np = Y_train.to_numpy()\n",
    "\n",
    "\n",
    "X_test = df_test.drop(labels, axis=1)\n",
    "Y_test = df_test[labels]\n",
    "\n",
    "X_test_np = X_test.to_numpy()\n",
    "Y_test_np = Y_test.to_numpy()\n",
    "\n",
    "\n",
    "def batch_generator(X, Y, batch_size, shuffle=True):\n",
    "    num_samples = X.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        X = X[indices]\n",
    "        Y = Y[indices]\n",
    "    for start_idx in range(0, num_samples, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, num_samples)\n",
    "        yield (X[start_idx:end_idx], Y[start_idx:end_idx])\n",
    "        \n",
    "        \n",
    "\n",
    "def hyperparameter_save(epochs,batch_size,hidden_size,learning_rate,reg_lambda,loss_values):\n",
    "    hyperparameters = {\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'hidden_size': hidden_size,\n",
    "            'learning_rate': learning_rate,\n",
    "            'reg_lambda': reg_lambda,\n",
    "            'losses': loss_values\n",
    "        }\n",
    "    with open('hyperparameters.json', 'a') as file:\n",
    "        json.dump(hyperparameters, file)\n",
    "        file.write('\\n')  # Add a newline to separate entries\n",
    "\n",
    "    read_hyperparameters_list = []\n",
    "    with open('hyperparameters.json', 'r') as file:\n",
    "        for line in file:\n",
    "            hyperparameters_dict = json.loads(line.strip())\n",
    "            read_hyperparameters_list.append(hyperparameters_dict)\n",
    "\n",
    "    for hyperparameters in read_hyperparameters_list:\n",
    "        print(hyperparameters)\n",
    "           \n",
    "    return read_hyperparameters_list        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMP 4437 Week 5 Lab Questions\n",
    "---\n",
    "#### Q1) Music Emotions dataset is given. Each entry of the dataset corresponds to a song. Each song can have multiple labels from:\n",
    "- amazed-suprised, \n",
    "- happy-pleased, \n",
    "- relaxing-calm, \n",
    "- quiet-still, \n",
    "- sad-lonely, \n",
    "- angry-aggresive\n",
    "\n",
    "<b> Implement <b style=\"color:red\">Multi Layer Perceptron</b> with  <b style=\"color:red\">2 Layers</b> for multilabel classification in a python class. Implement training loops and validation loops in the appropriate cells.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> Check the last week implementation of Two Layer Perceptron for binary classification\n",
    "</div>\n",
    "    \n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>!!</b> Make sure to use <b>X_train_np, Y_train_np, X_test_np, Y_test_np</b> for training and testing. Check above cell. \n",
    "</div> \n",
    "    \n",
    "---    \n",
    "#### Q2) Implement a searching method to figure out the best hyperparameters for this problem. Make sure to save each test run with the code given. Expected accuracy is atleast  <b style=\"color:red\">35%</b>\n",
    "    \n",
    "---    \n",
    "<br>    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerMLP:\n",
    "    \n",
    "\"\"\"\n",
    "######\n",
    "Implement Multi Layer Perceptron with 2 Layers for multilabel classification\n",
    "######\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    #Leave it as is\n",
    "    def __init__(self, model, learning_rate, batch_size):\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "     \n",
    "    def step(self):\n",
    "        self.model.update_weights(self.learning_rate)\n",
    "        \n",
    "    def zero_grad(self):\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_loop(model, optimizer, X_train, Y_train, epochs, batch_size):\n",
    "    \"\"\"\n",
    "    ######\n",
    "    Implement the training loop\n",
    "    ######\n",
    "    \"\"\"\n",
    "    return loss    \n",
    "\n",
    "def validate_loop(model, X_val, Y_val):\n",
    "    \"\"\"\n",
    "    ######\n",
    "    Implement the validation loop print the accuracy\n",
    "    ######\n",
    "    \n",
    "    # Use this formula for accuracy calculation\n",
    "    numerator = np.sum(np.logical_and(Y_val,y_pred))\n",
    "    demoninator = np.sum(np.logical_or(Y_val,y_pred))\n",
    "    accuracy = np.mean(numerator/demoninator)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return predictions, Y_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = None\n",
    "batch_size = None\n",
    "hidden_size = None\n",
    "learning_rate = None\n",
    "reg_lambda = None\n",
    "input_size=72\n",
    "output_size= 6\n",
    "loss_values = None\n",
    "\n",
    "\n",
    "#model = TwoLayerMLP(input_size, hidden_size, output_size) \n",
    "#optimizer = SGD(model, learning_rate)\n",
    "#loss_values=train_loop(model, optimizer, X_train_np, Y_train_np, epochs, batch_size)\n",
    "#predictions, Y_val = validate_loop(model, X_test_np, Y_test_np)\n",
    "\n",
    "np.random.seed(42)  # For reproducible results. Do not change\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "######\n",
    "Solve Question 2 Here\n",
    "\n",
    "\n",
    "### Do this for every hyperparameter test\n",
    "read_hyperparameters_list=hyperparameter_save(epochs,batch_size,hidden_size,learning_rate,reg_lambda,loss_values):\n",
    "\n",
    "\n",
    "######\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run For Plotting each hyperparameter test on a different subplot\"\"\"\n",
    "fig, axes = plt.subplots(len(read_hyperparameters_list), 1, figsize=(8, 6 * len(read_hyperparameters_list)))\n",
    "\n",
    "for idx, hyperparameters in enumerate(read_hyperparameters_list):\n",
    "    losses = hyperparameters['losses']\n",
    "    epochs = hyperparameters['epochs']\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    hidden_size = hyperparameters['hidden_size']\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    reg_lambda = hyperparameters['reg_lambda']\n",
    "\n",
    "    ax = axes[idx] if len(read_hyperparameters_list) > 1 else axes\n",
    "    ax.plot(losses)\n",
    "    ax.set_title(f'Epochs: {epochs}, Batch Size: {batch_size}, Hidden Size: {hidden_size}, Learning Rate: {learning_rate}, Reg Lambda: {reg_lambda}')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run For Plotting each hyperparameter test on a single\"\"\"\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "for idx, hyperparameters in enumerate(read_hyperparameters_list):\n",
    "    losses = hyperparameters['losses']\n",
    "    epochs = hyperparameters['epochs']\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    hidden_size = hyperparameters['hidden_size']\n",
    "    learning_rate = hyperparameters['learning_rate']\n",
    "    reg_lambda = hyperparameters['reg_lambda']\n",
    "\n",
    "    ax.plot(losses, label=f'Run {idx + 1}')\n",
    "    ax.text(len(losses) - 1, losses[-1], f'Epochs={epochs}\\nBatch Size={batch_size}\\nHidden Size={hidden_size}\\\n",
    "    \\nLearning Rate={learning_rate}\\nReg Lambda={reg_lambda}', ha='center', va='center', fontsize=8)\n",
    "\n",
    "ax.set_title('Loss Curves for Different Hyperparameter Settings')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
