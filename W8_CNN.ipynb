{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bvG4Lavq99Gq",
      "metadata": {
        "id": "bvG4Lavq99Gq"
      },
      "source": [
        "## COMP 4437 WEEK 8 Assignment\n",
        "\n",
        "The task in this assignment is hand-written digit recognition, i.e., a classifier must predict the value of the digit given its image. You are provided the codes to load the dataset with on-the-fly pre-processing transformation, evaluate the performance, and plot the learning curves. The batch size is fixed to 64 with the DataLoader objects. You will build an MLP model and a CNN model. You are free in your network design, choice of loss function, and optimizer selection. Your implementation must be by PyTorch.\n",
        "\n",
        "1. Design a simple MLP by implementing the class SimpleMLP. Then write the training and validation code. You MUST have at least 95% accuracy on the validation set.\n",
        "2. Design a simple CNN by implementing the class SimpleCNN. Then write the training and validation code. You MUST have at least 96% accuracy on the validation set (a simple CNN is expected to work better than a simple MLP in this task).\n",
        "\n",
        "Suggestion: Standard convolution blocks would be fine, i.e., convolution -> Max. Pooling -> ReLU. You can repeat this block and at the final stage, you can put just a single linear layer or, if you want, an MLP, on top of the convolutional layers.\n",
        "\n",
        "Remember, your implementations must utilize **PyTorch's functionalities**, and you should justify your choices for the network architectures, loss function, and optimizer. Consider the trade-offs or benefits your selections might have on the performance and efficiency of the training process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cPbg9gnz99Gs",
      "metadata": {
        "id": "cPbg9gnz99Gs"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "omdKjysNM3Fh",
      "metadata": {
        "id": "omdKjysNM3Fh"
      },
      "outputs": [],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "X9HHfT4N99Gt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9HHfT4N99Gt",
        "outputId": "49f216ff-8585-494c-b077-21b168d9e704"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "validationset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "validationloader = torch.utils.data.DataLoader(validationset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "WkHKBqGz99Gt",
      "metadata": {
        "id": "WkHKBqGz99Gt"
      },
      "outputs": [],
      "source": [
        "def plot_losses(train_losses, val_losses, num_epochs):\n",
        "    \"\"\"\n",
        "    Plots the training and validation losses.\n",
        "\n",
        "    Parameters:\n",
        "    - train_losses (list of float): A list containing the training loss values.\n",
        "    - val_losses (list of float): A list containing the validation loss values.\n",
        "    - num_epochs (int): The number of epochs the model was trained for.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
        "    plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "q7cPQxfK99Gu",
      "metadata": {
        "id": "q7cPQxfK99Gu"
      },
      "outputs": [],
      "source": [
        "def plot_accuracies(train_accuracies, val_accuracies, num_epochs):\n",
        "    \"\"\"\n",
        "    Plots the training and validation accuracies.\n",
        "\n",
        "    Parameters:\n",
        "    - train_accuracies (list of float): A list containing the training accuracy values.\n",
        "    - val_accuracies (list of float): A list containing the validation accuracy values.\n",
        "    - num_epochs (int): The number of epochs the model was trained for.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(range(1, num_epochs + 1), train_accuracies, label='Training Accuracy')\n",
        "    plt.plot(range(1, num_epochs + 1), val_accuracies, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Training and Validation Accuracies')\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "lkh_fgKr99Gu",
      "metadata": {
        "id": "lkh_fgKr99Gu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_class_accuracies(class_correct, class_total, classes):\n",
        "    \"\"\"\n",
        "    Plots the class-wise accuracies as a bar chart.\n",
        "\n",
        "    Parameters:\n",
        "    - class_correct (list): A list containing the number of correct predictions for each class.\n",
        "    - class_total (list): A list containing the total number of predictions for each class.\n",
        "    - classes (list): A list of classes.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    class_accuracies = [class_correct[i] / class_total[i] for i in classes]\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.bar(classes, class_accuracies)\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Class-wise Accuracies')\n",
        "    plt.xticks(classes)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1qVkH1kT99Gu",
      "metadata": {
        "id": "1qVkH1kT99Gu"
      },
      "source": [
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "KQM5by-L99Gu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQM5by-L99Gu",
        "outputId": "8de4eae8-e598-4d1b-e818-c43adcac54f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.1150, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A simple Multi-Layer Perceptron (MLP) for classification tasks that\n",
        "  inherits from nn.Module, which is a base class for all neural network modules in PyTorch.\n",
        "\n",
        "  Initialize the SimpleMLP model by setting up the layers.\n",
        "\n",
        "  Define the forward pass of the MLP.\n",
        "          Parameters:\n",
        "          - x (Tensor): The input tensor containing the batch of images. The images\n",
        "            must be flattened before being passed to the network.\n",
        "\n",
        "          Returns:\n",
        "          - x (Tensor): The output tensor containing the logits for each class in the\n",
        "            batch. To convert these logits to probabilities, apply a softmax function\n",
        "            outside this method.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.linear_relu_stack = nn.Sequential(\n",
        "          nn.Linear(28*28, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 512),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(512, 10),\n",
        "          nn.Softmax(1)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.flatten(x)\n",
        "      logits = self.linear_relu_stack(x)\n",
        "      return logits\n",
        "\n",
        "\n",
        "mlp = SimpleMLP()\n",
        "\n",
        "a = mlp.forward(trainset[0][0])\n",
        "\n",
        "print(torch.max(a))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9X50Rn2E99Gu",
      "metadata": {
        "id": "9X50Rn2E99Gu"
      },
      "outputs": [],
      "source": [
        "class SimpleCNN(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "  A simple CNN for classification tasks that\n",
        "  inherits from nn.Module, which is a base class for all neural network modules in PyTorch.\n",
        "\n",
        "  Initialize the SimpleCNN model by setting up the layers.\n",
        "\n",
        "  Define the forward pass of the CNN.\n",
        "          Parameters:\n",
        "          - x (Tensor): The input tensor containing the batch of images. The images\n",
        "            must be flattened before being passed to the network.\n",
        "\n",
        "          Returns:\n",
        "          - x (Tensor): The output tensor containing the logits for each class in the\n",
        "            batch. To convert these logits to probabilities, apply a softmax function\n",
        "            outside this method.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self):\n",
        "      super().__init__()\n",
        "      self.flatten = nn.Flatten()\n",
        "      self.conv1 = nn.Conv2d(1,32,5,1,2)\n",
        "      self.conv2 =nn.Conv2d(32,64,5,1,2)\n",
        "      self.fc1 = nn.Linear(64 * 7*7, 512)\n",
        "      self.fc2 = nn.Linear(512, 10)\n",
        "      self.pool = nn.MaxPool2d(2,2,0)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(F.relu(self.conv1(x)))\n",
        "      x = self.pool(F.relu(self.conv2(x)))\n",
        "      x = self.flatten(x)\n",
        "      x = F.relu(self.fc1(x))\n",
        "      x = self.fc2(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "cnn = SimpleCNN().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "AblAtsE-99Gu",
      "metadata": {
        "id": "AblAtsE-99Gu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Add criterion (loss function) and optimizer\n",
        "\"\"\"\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cnn.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ck-XSvA999Gu",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck-XSvA999Gu",
        "outputId": "b071014d-01e0-4871-c622-8e990ca40ef4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.296448  [   64/60000]\n",
            "loss: 0.070572  [ 6464/60000]\n",
            "loss: 0.135450  [12864/60000]\n",
            "loss: 0.011585  [19264/60000]\n",
            "loss: 0.112728  [25664/60000]\n",
            "loss: 0.092556  [32064/60000]\n",
            "loss: 0.043416  [38464/60000]\n",
            "loss: 0.038307  [44864/60000]\n",
            "loss: 0.014449  [51264/60000]\n",
            "loss: 0.018752  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.036713 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.104605  [   64/60000]\n",
            "loss: 0.026199  [ 6464/60000]\n",
            "loss: 0.043216  [12864/60000]\n",
            "loss: 0.010360  [19264/60000]\n",
            "loss: 0.029768  [25664/60000]\n",
            "loss: 0.019513  [32064/60000]\n",
            "loss: 0.088705  [38464/60000]\n",
            "loss: 0.061155  [44864/60000]\n",
            "loss: 0.025440  [51264/60000]\n",
            "loss: 0.012512  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.035795 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.001348  [   64/60000]\n",
            "loss: 0.001878  [ 6464/60000]\n",
            "loss: 0.014097  [12864/60000]\n",
            "loss: 0.007091  [19264/60000]\n",
            "loss: 0.029405  [25664/60000]\n",
            "loss: 0.099096  [32064/60000]\n",
            "loss: 0.001303  [38464/60000]\n",
            "loss: 0.001269  [44864/60000]\n",
            "loss: 0.059286  [51264/60000]\n",
            "loss: 0.003627  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.8%, Avg loss: 0.043639 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.114334  [   64/60000]\n",
            "loss: 0.003850  [ 6464/60000]\n",
            "loss: 0.064489  [12864/60000]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     test(validationloader, cnn, loss_fn)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[11], line 21\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32mc:\\Users\\Seden\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Seden\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "classes = range(10)\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(trainloader, cnn, loss_fn, optimizer)\n",
        "    test(validationloader, cnn, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "\"\"\"\n",
        "    Epochs: An epoch is a single pass through the entire training dataset.\n",
        "    The code is set to run for a total of 10 epochs.\n",
        "\n",
        "    Loss and Accuracy Tracking: Separate lists are maintained to track training and validation losses\n",
        "    (train_losses and val_losses) as well as accuracies (train_accuracies and val_accuracies).\n",
        "\n",
        "    Class-wise Correct Predictions: class_correct and class_total lists are used to track the\n",
        "    number of correct predictions and the total number of predictions, respectively, for each class\n",
        "    across all validation batches.\n",
        "\n",
        "\n",
        "    Training Phase (per epoch)\n",
        "    #Mode Setting: The network is set to training mode (net.train()) to enable dropout\n",
        "    layers and batch normalization layers to function in training mode.\n",
        "    #Batch Processing: The training dataset is processed in batches. For each batch:\n",
        "        #Forward Pass: The network computes the output based on the input data.\n",
        "        #Loss Calculation: The loss is calculated using a criterion comparing the output and the true labels.\n",
        "        #Backward Pass and Optimization: The gradients are calculated by backpropagating the loss,\n",
        "        and the optimizer updates the model's parameters.\n",
        "        #Accuracy Tracking: The total and correct predictions are tracked to calculate the accuracy for the epoch.\n",
        "    #Loss and Accuracy Calculation: The average loss and accuracy for the epoch are appended to their respective lists.\n",
        "\n",
        "\n",
        "\n",
        "    Validation Phase (per epoch)\n",
        "\n",
        "    #Mode Setting: The network is set to evaluation mode (net.eval()) to disable dropout layers\n",
        "    and batch normalization layers from updating, ensuring the model's behavior is consistent for evaluation.\n",
        "    #Batch Processing: Similar to the training phase but with key differences:\n",
        "        #No Gradient Calculations: Gradients are not computed (torch.no_grad()),\n",
        "        which reduces memory consumption and speeds up the process.\n",
        "        #Loss and Accuracy Tracking: As in training, but also includes class-wise accuracy calculations.\n",
        "        #Class-wise Accuracy: For each prediction that matches the true label,\n",
        "        the count of correct predictions per class is updated.\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "wKDIdrM-99Gv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "wKDIdrM-99Gv",
        "outputId": "ca913c26-9380-409f-b0e0-d1123b633069"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-7e636819711f>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \"\"\" \n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidationloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-7e636819711f>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(loader, network)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Do not compute gradient since we're only predicting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get the index of the max log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert predictions to numpy and add to list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-aedd712be71a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def evaluate_model(loader, network):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of a neural network model on a dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - loader (DataLoader): A DataLoader object that provides batches of the dataset for evaluation.\n",
        "      It should yield two elements: inputs and labels.\n",
        "    - network (nn.Module): The neural network model to be evaluated. This can be any model that is a\n",
        "      subclass of torch.nn.Module, such as CNNs, MLPs, etc.\n",
        "\n",
        "    Returns:\n",
        "    - accuracy (float): The accuracy of the model on the dataset, defined as the proportion of\n",
        "      correct predictions over the total number of instances evaluated.\n",
        "    - precision (float): The macro-averaged precision of the model, which calculates precision\n",
        "      (the ratio of true positive predictions to the total positive predictions) for each label,\n",
        "      and then averages these values.\n",
        "    - recall (float): The macro-averaged recall of the model, which calculates recall\n",
        "      (the ratio of true positive predictions to the actual positive instances) for each label,\n",
        "      and then averages these values.\n",
        "    - f1 (float): The macro-averaged F1 score of the model, which is the harmonic mean of precision\n",
        "      and recall. The F1 score is particularly useful in situations where there is an uneven class\n",
        "      distribution or when false positives and false negatives have different costs.\n",
        "\n",
        "    The function computes these metrics by first setting the model to evaluation mode and then\n",
        "    iterating over the dataset, collecting the model's predictions. It leverages PyTorch's\n",
        "    torch.no_grad() context to avoid calculating gradients during evaluation, which saves memory\n",
        "    and computational resources. The sklearn.metrics library is used to compute the final metrics\n",
        "    from the true labels and the predicted labels.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    net = network\n",
        "    net.eval()  # Set the model to evaluation mode\n",
        "    y_true = []  # List to store the true labels\n",
        "    y_pred = []  # List to store the predicted labels\n",
        "\n",
        "    with torch.no_grad():  # Do not compute gradient since we're only predicting\n",
        "        for inputs, labels in loader:\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
        "            y_pred.extend(predicted.numpy())  # Convert predictions to numpy and add to list\n",
        "            y_true.extend(labels.numpy())  # Convert true labels to numpy and add to list\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
        "\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Note: Change the 'network' parameter name as appropriate for the model being evaluated, (cnn, mlp).\n",
        "\"\"\"\n",
        "\n",
        "accuracy, precision, recall, f1 = evaluate_model(validationloader, cnn)\n",
        "print(f'Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4BAQX8U399Gv",
      "metadata": {
        "id": "4BAQX8U399Gv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Call Plotting Functions Here\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
